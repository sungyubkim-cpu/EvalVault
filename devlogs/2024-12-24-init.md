# EvalVault 개발 로그 - 2024-12-24

## 프로젝트 개요

RAG 평가 시스템 (Ragas + Langfuse) 구축

### 핵심 흐름
```
입력(CSV/Excel) → Ragas 평가 → Langfuse trace/score 저장 → 분석
```

## 진행 상황

### Phase 1: 프로젝트 초기화 ✅

1. **uv 초기화 및 pyproject.toml 설정**
   - Python 3.12+
   - 의존성: ragas, langfuse, openai, pydantic, typer, pandas, openpyxl, structlog

2. **디렉토리 구조 생성** (Hexagonal Architecture)
   ```
   src/evalvault/
   ├── domain/entities/     # 비즈니스 엔티티
   ├── domain/services/     # 비즈니스 로직
   ├── ports/inbound/       # 입력 인터페이스
   ├── ports/outbound/      # 출력 인터페이스
   ├── adapters/inbound/    # CLI, HTTP
   ├── adapters/outbound/   # LLM, Dataset, Storage, Tracker
   └── config/              # 설정
   ```

3. **Domain 엔티티 정의** ✅
   - `TestCase`: 단일 평가 케이스 (Ragas SingleTurnSample 매핑)
   - `Dataset`: 테스트 케이스 컬렉션
   - `MetricScore`: 개별 메트릭 점수
   - `TestCaseResult`: 개별 케이스 평가 결과
   - `EvaluationRun`: 전체 평가 실행 결과

4. **단위 테스트 작성 및 통과** ✅
   - 19개 테스트 모두 통과
   - TDD 원칙 적용

---

## 병렬 작업 계획 (4 에이전트)

### Agent 1: Port 인터페이스 정의
- `ports/outbound/llm_port.py` - LLM 호출 인터페이스
- `ports/outbound/dataset_port.py` - 데이터셋 로드 인터페이스
- `ports/outbound/storage_port.py` - 결과 저장 인터페이스
- `ports/outbound/tracker_port.py` - Langfuse 추적 인터페이스
- `ports/inbound/evaluator_port.py` - 평가 실행 인터페이스

### Agent 2: 데이터 로더 구현
- `adapters/outbound/dataset/csv_loader.py` - CSV 로더
- `adapters/outbound/dataset/excel_loader.py` - Excel 로더
- `adapters/outbound/dataset/json_loader.py` - JSON 로더
- 테스트 코드 작성

### Agent 3: Ragas 평가 서비스
- `domain/services/evaluator.py` - 평가 로직
- `adapters/outbound/llm/openai_adapter.py` - OpenAI 어댑터
- Ragas 메트릭 통합 (Faithfulness, Answer Relevancy, Context Precision, Context Recall)
- 테스트 코드 작성

### Agent 4: Langfuse 트래커 어댑터
- `adapters/outbound/tracker/langfuse_adapter.py` - Langfuse 어댑터
- trace 생성, score 적재 기능
- artifact 저장 기능
- 테스트 코드 작성

---

## Phase 2: 병렬 에이전트 작업 ✅

4개 에이전트가 병렬로 작업을 수행하여 모든 핵심 기능 구현 완료.

### Agent 1: Port 인터페이스 정의 ✅
- `LLMPort` (ABC) - Ragas 호환 LLM 인터페이스
- `DatasetPort` - 데이터셋 로드 인터페이스
- `StoragePort` - 결과 저장 인터페이스
- `TrackerPort` - Langfuse 추적 인터페이스
- `EvaluatorPort` - 평가 실행 인터페이스
- **24개 테스트 통과**

### Agent 2: 데이터 로더 구현 ✅
- `CSVDatasetLoader` - CSV 파일 로드 (JSON/파이프 구분 contexts 지원)
- `ExcelDatasetLoader` - Excel 파일 로드
- `JSONDatasetLoader` - JSON 파일 로드
- `get_loader()` - 팩토리 함수
- **21개 테스트 통과**

### Agent 3: Ragas 평가 서비스 ✅
- `RagasEvaluator` - Ragas 기반 평가 서비스
- `OpenAIAdapter` - LangChain ChatOpenAI 래퍼
- `Settings` - pydantic-settings 기반 설정
- 지원 메트릭: faithfulness, answer_relevancy, context_precision, context_recall
- **11개 테스트 통과**

### Agent 4: Langfuse 트래커 ✅
- `LangfuseAdapter` - Langfuse SDK 래퍼
- trace/span 생성, score 로깅, artifact 저장
- `log_evaluation_run()` - EvaluationRun 전체 로깅
- 셀프호스팅 지원 (`langfuse_host` 설정)
- **18개 테스트 통과**

---

## Phase 3: CLI 및 통합 테스트 ✅

### CLI 인터페이스 (`evalvault` 명령어)
```bash
# 평가 실행
evalvault run data.csv --metrics faithfulness,answer_relevancy

# 사용 가능한 메트릭 목록
evalvault metrics

# 현재 설정 확인
evalvault config
```

### 통합 테스트
- 데이터 플로우 테스트 (CSV/Excel/JSON → Ragas 형식)
- 평가 플로우 테스트 (Mock LLM)
- Langfuse 플로우 테스트 (Mock)
- API 키가 있을 때만 실행되는 실제 API 테스트 (`@pytest.mark.requires_openai`)

---

## 테스트 현황

| 구분 | 테스트 수 | 상태 |
|------|----------|------|
| 단위 테스트 | 99개 | ✅ 통과 |
| 통합 테스트 | 17개 | ✅ 통과 |
| 스킵 (API 미설정) | 2개 | ⏭️ |
| **합계** | **116개** | **통과** |

---

## 환경 설정

`.env` 파일 예시:
```bash
# OpenAI (필수)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# Langfuse (셀프호스팅)
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
LANGFUSE_HOST=http://localhost:3000

# 메트릭 임계값
THRESHOLD_FAITHFULNESS=0.7
THRESHOLD_ANSWER_RELEVANCY=0.7
```

---

## 다음 단계 (IMPLEMENTATION_PLAN.md 반영)

### 진행 중
- [ ] 실제 OpenAI API로 E2E 테스트
- [ ] Langfuse 셀프호스팅 연동 테스트

### Phase 4: 기반 강화 (P0)
- [ ] 언어 감지 유틸리티 (`langdetect` 라이브러리 사용)
- [ ] 한국어 프롬프트 커스터마이징 (Faithfulness, AnswerRelevancy)
- [ ] FactualCorrectness 메트릭 추가
- [ ] SemanticSimilarity 메트릭 추가

### Phase 5: 보험 도메인 특화 (P1)
- [ ] InsuranceTermAccuracy 메트릭 (AspectCritic 기반)
- [ ] 기본 Testset Generation
- [ ] 결과 저장 어댑터 (StoragePort 구현체 - SQLite)

### Phase 6: 고급 기능 (P2)
- [ ] Knowledge Graph 기반 Testset 생성
- [ ] Experiment 관리 시스템
- [ ] 다국어 프롬프트 확장 (중국어, 일본어)

---

## IMPLEMENTATION_PLAN.md 비판적 평가

### 반영된 항목
- 언어 감지 유틸리티 (단, `langdetect` 라이브러리 사용으로 단순화)
- 한국어 프롬프트 커스터마이징
- FactualCorrectness, SemanticSimilarity 메트릭
- InsuranceTermAccuracy 메트릭 (AspectCritic 기반)
- Testset Generation 기본 개념

### 보류/제외 항목
- Unicode 범위 기반 언어 감지 (불안정, `langdetect` 사용)
- Knowledge Graph 기반 복잡한 테스트셋 생성 (과설계)
- 복잡한 페르소나/쿼리 스타일 시스템 (과설계)
- @experiment 데코레이터 통합 (현재 어댑터 패턴으로 충분)
- 시간 추정 ("Week 1-2" 등 - 정책상 제거)
