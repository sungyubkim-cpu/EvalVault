# EvalVault êµ¬í˜„ ê³„íšì„œ

> í•œêµ­ì–´/ì˜ì–´ ë³´í—˜ ë¬¸ì„œ RAG í‰ê°€ ì‹œìŠ¤í…œ - RAGAS + Langfuse ê¸°ë°˜

## 1. í˜„ì¬ ìƒíƒœ ë¶„ì„

### 1.1 êµ¬í˜„ ì™„ë£Œëœ ë¶€ë¶„

| ì»´í¬ë„ŒíŠ¸ | ìƒíƒœ | íŒŒì¼ |
|---------|------|------|
| Domain Entities | âœ… ì™„ë£Œ | `dataset.py`, `result.py` |
| RagasEvaluator | âœ… ê¸°ë³¸ êµ¬í˜„ | `evaluator.py` |
| Langfuse Adapter | âœ… ì™„ë£Œ | `langfuse_adapter.py` |
| Dataset Loaders | âœ… CSV/JSON/Excel | `adapters/outbound/dataset/` |
| OpenAI Adapter | âœ… ì™„ë£Œ | `openai_adapter.py` |

### 1.2 í˜„ì¬ ì§€ì› ë©”íŠ¸ë¦­

```python
METRIC_MAP = {
    "faithfulness": Faithfulness,
    "answer_relevancy": AnswerRelevancy,
    "context_precision": ContextPrecision,
    "context_recall": ContextRecall,
}
```

### 1.3 ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­

1. **ë‹¤êµ­ì–´(í•œêµ­ì–´/ì˜ì–´) ì§€ì›**: í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”
2. **ë³´í—˜ ë„ë©”ì¸ íŠ¹í™” ë©”íŠ¸ë¦­**: ë„ë©”ì¸ ìš©ì–´, ê·œì • ì¤€ìˆ˜ í‰ê°€
3. **Testset Generation**: ë³´í—˜ ë¬¸ì„œ íŠ¹í™” í…ŒìŠ¤íŠ¸ì…‹ ìë™ ìƒì„±
4. **Experiment ê´€ë¦¬**: RAGAS @experiment ë°ì½”ë ˆì´í„° í†µí•©
5. **ê³ ê¸‰ ë©”íŠ¸ë¦­**: FactualCorrectness, SemanticSimilarity ë“± ì¶”ê°€

---

## 2. ë‹¤êµ­ì–´(Multilingual) ì§€ì› ì „ëµ

### 2.1 ì§€ì› ì–¸ì–´ ìš°ì„ ìˆœìœ„

ë³´í—˜ ë¬¸ì„œì— í¬í•¨ë  ìˆ˜ ìˆëŠ” ì–¸ì–´ì˜ ìš°ì„ ìˆœìœ„:

| ìš°ì„ ìˆœìœ„ | ì–¸ì–´ | ì½”ë“œ | ì˜ˆìƒ ë¹„ìœ¨ | ë¹„ê³  |
|---------|------|------|----------|------|
| 1ìˆœìœ„ | í•œêµ­ì–´ | `ko` | ~70% | ì£¼ìš” ì–¸ì–´ |
| 2ìˆœìœ„ | ì˜ì–´ | `en` | ~20% | ë³´ì¡° ì–¸ì–´, ê¸°ìˆ  ìš©ì–´ |
| 3ìˆœìœ„ | ì¤‘êµ­ì–´ | `zh` | ~5% | ì¤‘êµ­ì¸ ê³ ê° ëŒ€ìƒ |
| 4ìˆœìœ„ | ì¼ë³¸ì–´ | `ja` | ~3% | ì¼ë³¸ì¸ ê³ ê° ëŒ€ìƒ |
| 5ìˆœìœ„ | ë² íŠ¸ë‚¨ì–´ | `vi` | ~2% | ë² íŠ¸ë‚¨ì¸ ê³ ê° ëŒ€ìƒ |
| ê¸°íƒ€ | ë‹¤êµ­ì–´ | `*` | - | í•„ìš”ì‹œ í™•ì¥ |

### 2.2 ì–¸ì–´ ê°ì§€ ìœ í‹¸ë¦¬í‹°

```python
# src/evalvault/utils/language.py

from dataclasses import dataclass
from typing import Literal

# ì§€ì› ì–¸ì–´ íƒ€ì…
SupportedLanguage = Literal["ko", "en", "zh", "ja", "vi", "other"]

# ì–¸ì–´ë³„ ìœ ë‹ˆì½”ë“œ ë²”ìœ„
LANGUAGE_RANGES = {
    "ko": [('\uac00', '\ud7af'), ('\u1100', '\u11ff')],  # í•œê¸€
    "zh": [('\u4e00', '\u9fff')],  # ì¤‘êµ­ì–´ (CJK)
    "ja": [('\u3040', '\u309f'), ('\u30a0', '\u30ff')],  # íˆë¼ê°€ë‚˜, ì¹´íƒ€ì¹´ë‚˜
    "vi": [('\u0100', '\u017f')],  # ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ë¬¸ì (Latin Extended)
}

@dataclass
class LanguageDetectionResult:
    """ì–¸ì–´ ê°ì§€ ê²°ê³¼."""
    primary: SupportedLanguage
    confidence: float
    distribution: dict[str, float]  # ì–¸ì–´ë³„ ë¹„ìœ¨


def detect_language(text: str) -> LanguageDetectionResult:
    """í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.

    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸

    Returns:
        ì–¸ì–´ ê°ì§€ ê²°ê³¼ (ì£¼ìš” ì–¸ì–´, ì‹ ë¢°ë„, ë¶„í¬)
    """
    if not text:
        return LanguageDetectionResult("en", 0.0, {})

    # ì–¸ì–´ë³„ ë¬¸ì ìˆ˜ ê³„ì‚°
    char_counts = {lang: 0 for lang in LANGUAGE_RANGES}
    total_chars = len([c for c in text if not c.isspace()])

    for char in text:
        for lang, ranges in LANGUAGE_RANGES.items():
            for start, end in ranges:
                if start <= char <= end:
                    char_counts[lang] += 1
                    break

    # ë¶„í¬ ê³„ì‚°
    distribution = {
        lang: count / total_chars if total_chars > 0 else 0
        for lang, count in char_counts.items()
    }

    # í•œêµ­ì–´ ìš°ì„  íŒì • (CJK ë²”ìœ„ ì¤‘ì²© ì²˜ë¦¬)
    if distribution.get("ko", 0) > 0.2:
        primary = "ko"
    elif distribution.get("ja", 0) > 0.1:
        primary = "ja"
    elif distribution.get("zh", 0) > 0.2:
        primary = "zh"
    elif distribution.get("vi", 0) > 0.05:
        primary = "vi"
    else:
        primary = "en"  # ê¸°ë³¸ê°’

    confidence = distribution.get(primary, 0) if primary != "en" else 1.0 - sum(distribution.values())

    return LanguageDetectionResult(
        primary=primary,
        confidence=confidence,
        distribution=distribution,
    )


def get_dominant_language(texts: list[str]) -> SupportedLanguage:
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì–¸ì–´ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."""
    if not texts:
        return "en"

    # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì³ì„œ ë¶„ì„
    combined = " ".join(texts)
    result = detect_language(combined)
    return result.primary


def is_mixed_language(text: str, threshold: float = 0.1) -> bool:
    """í˜¼í•© ì–¸ì–´ ë¬¸ì„œì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.

    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        threshold: í˜¼í•©ìœ¼ë¡œ íŒë‹¨í•  ìµœì†Œ ë¹„ìœ¨

    Returns:
        True if 2ê°œ ì´ìƒ ì–¸ì–´ê°€ threshold ì´ìƒ í¬í•¨
    """
    result = detect_language(text)
    significant_languages = [
        lang for lang, ratio in result.distribution.items()
        if ratio >= threshold
    ]
    return len(significant_languages) >= 2
```

### 2.3 ë‹¤êµ­ì–´ í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

RAGASì˜ LLM ê¸°ë°˜ ë©”íŠ¸ë¦­ì€ `Prompt Object`ë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# src/evalvault/domain/prompts/multilingual_prompts.py

from ragas.metrics import Faithfulness
from evalvault.utils.language import SupportedLanguage, detect_language

class MultilingualFaithfulness(Faithfulness):
    """ë‹¤êµ­ì–´ ë¬¸ì„œë¥¼ ìœ„í•œ Faithfulness ë©”íŠ¸ë¦­."""

    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    PROMPTS = {
        "ko": {
            "nli": "ë‹¤ìŒ ì£¼ì¥ì´ ì£¼ì–´ì§„ ë§¥ë½ì—ì„œ ì§€ì§€ë˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”...",
            "instruction": "ë‹µë³€ì˜ ê° ë¬¸ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.",
        },
        "en": {
            "nli": "Determine if the following claim is supported by the context...",
            "instruction": "Evaluate if each sentence in the answer is grounded in the context.",
        },
        "zh": {
            "nli": "åˆ¤æ–­ä»¥ä¸‹å£°æ˜æ˜¯å¦å¾—åˆ°ä¸Šä¸‹æ–‡çš„æ”¯æŒ...",
            "instruction": "è¯„ä¼°ç­”æ¡ˆä¸­çš„æ¯ä¸ªå¥å­æ˜¯å¦åŸºäºä¸Šä¸‹æ–‡ã€‚",
        },
        "ja": {
            "nli": "ä»¥ä¸‹ã®ä¸»å¼µãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã‚ˆã£ã¦æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„...",
            "instruction": "å›ç­”ã®å„æ–‡ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚",
        },
    }

    def __init__(self, language: SupportedLanguage = "auto"):
        super().__init__()
        self.language = language

    def _get_prompt_for_language(self, lang: SupportedLanguage) -> dict:
        """ì–¸ì–´ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ë°˜í™˜."""
        return self.PROMPTS.get(lang, self.PROMPTS["en"])

    async def single_turn_ascore(self, sample):
        # ì–¸ì–´ ìë™ ê°ì§€
        if self.language == "auto":
            detected = detect_language(sample.response)
            lang = detected.primary
        else:
            lang = self.language

        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        prompts = self._get_prompt_for_language(lang)
        self._customize_prompts(prompts)

        return await super().single_turn_ascore(sample)
```

### 2.4 êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ìš°ì„ ìˆœìœ„ | ì‘ì—… | ì„¤ëª… |
|---------|------|------|
| P0 | ì–¸ì–´ ê°ì§€ ìœ í‹¸ë¦¬í‹° | ë‹¤êµ­ì–´ ìë™ ê°ì§€ (ko, en, zh, ja, vi) |
| P0 | í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ | Faithfulness, AnswerRelevancy í•œêµ­ì–´í™” |
| P0 | ì˜ì–´ í”„ë¡¬í”„íŠ¸ | ê¸°ë³¸ RAGAS í”„ë¡¬í”„íŠ¸ (ì´ë¯¸ ì§€ì›) |
| P1 | í˜¼í•© ì–¸ì–´ ì²˜ë¦¬ | í•œì˜ í˜¼í•© ë¬¸ì„œ ë“± ì²˜ë¦¬ ì „ëµ |
| P1 | ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í”„ë¡¬í”„íŠ¸ | 3ìˆœìœ„/4ìˆœìœ„ ì–¸ì–´ ì§€ì› |
| P2 | ë² íŠ¸ë‚¨ì–´ í”„ë¡¬í”„íŠ¸ | 5ìˆœìœ„ ì–¸ì–´ ì§€ì› |
| P2 | ë™ì  ì–¸ì–´ ì „í™˜ | ë¬¸ì„œë³„ ìë™ í”„ë¡¬í”„íŠ¸ ì „í™˜ |
| P3 | ê¸°íƒ€ ì–¸ì–´ í™•ì¥ | í•„ìš”ì‹œ ì¶”ê°€ ì–¸ì–´ ì§€ì› |

### 2.5 í˜¼í•© ì–¸ì–´ ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ

ë³´í—˜ ë¬¸ì„œì—ì„œ í”íˆ ë‚˜íƒ€ë‚˜ëŠ” í˜¼í•© ì–¸ì–´ íŒ¨í„´:

```
íŒ¨í„´ 1: í•œê¸€ + ì˜ì–´ ê¸°ìˆ ìš©ì–´
  ì˜ˆ: "ì´ ë³´í—˜ì˜ deductible(ìê¸°ë¶€ë‹´ê¸ˆ)ì€ 10ë§Œì›ì…ë‹ˆë‹¤."

íŒ¨í„´ 2: í•œê¸€ + ì˜ì–´ ë¸Œëœë“œëª…
  ì˜ˆ: "Samsung Life Insurance ì¢…ì‹ ë³´í—˜ ìƒí’ˆ"

íŒ¨í„´ 3: ë‹¤êµ­ì–´ ì•½ê´€
  ì˜ˆ: ì˜ë¬¸/ì¤‘ë¬¸/êµ­ë¬¸ ë³‘ê¸° ì•½ê´€
```

```python
# í˜¼í•© ì–¸ì–´ ì²˜ë¦¬ ì „ëµ
class MixedLanguageStrategy:
    """í˜¼í•© ì–¸ì–´ ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ."""

    def process(self, text: str) -> ProcessedText:
        if is_mixed_language(text):
            # ì£¼ìš” ì–¸ì–´ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ë˜, ë³´ì¡° ì–¸ì–´ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
            primary = get_dominant_language([text])
            return ProcessedText(
                text=text,
                primary_language=primary,
                evaluation_language=primary,  # ì£¼ìš” ì–¸ì–´ë¡œ í‰ê°€
                preserve_foreign_terms=True,   # ì™¸ë˜ì–´ ë³´ì¡´
            )
        else:
            primary = detect_language(text).primary
            return ProcessedText(
                text=text,
                primary_language=primary,
                evaluation_language=primary,
            )
```

---

## 3. ë³´í—˜ ë„ë©”ì¸ íŠ¹í™” ë©”íŠ¸ë¦­

### 3.1 ì¶”ê°€ ë©”íŠ¸ë¦­ ëª©ë¡

RAGAS Core Conceptsì—ì„œ ì œê³µí•˜ëŠ” ë©”íŠ¸ë¦­ ì¤‘ ë³´í—˜ ë„ë©”ì¸ì— ì í•©í•œ ê²ƒë“¤:

#### RAG í’ˆì§ˆ ë©”íŠ¸ë¦­ (ê¸°ì¡´ + ì¶”ê°€)

| ë©”íŠ¸ë¦­ | ìš©ë„ | ìš°ì„ ìˆœìœ„ |
|--------|------|----------|
| `Faithfulness` | ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ | âœ… êµ¬í˜„ë¨ |
| `AnswerRelevancy` | ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ | âœ… êµ¬í˜„ë¨ |
| `ContextPrecision` | ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë°€ë„ | âœ… êµ¬í˜„ë¨ |
| `ContextRecall` | í•„ìš”í•œ ì •ë³´ê°€ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ | âœ… êµ¬í˜„ë¨ |
| `FactualCorrectness` | ì‚¬ì‹¤ì  ì •í™•ì„± (ë³´í—˜ ê·œì •) | P0 |
| `ContextEntitiesRecall` | ì—”í‹°í‹° ìˆ˜ì¤€ recall | P1 |
| `NoiseSensitivity` | ë…¸ì´ì¦ˆì— ëŒ€í•œ ë¯¼ê°ë„ | P2 |

#### ìì—°ì–´ ë¹„êµ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ìš©ë„ | ìš°ì„ ìˆœìœ„ |
|--------|------|----------|
| `SemanticSimilarity` | ì˜ë¯¸ì  ìœ ì‚¬ë„ | P0 |
| `RougeScore` | ìš”ì•½ í’ˆì§ˆ (ë³´í—˜ ìš”ì•½) | P1 |
| `BleuScore` | ë²ˆì—­ í’ˆì§ˆ | P2 |

### 3.2 ì»¤ìŠ¤í…€ ë³´í—˜ ë„ë©”ì¸ ë©”íŠ¸ë¦­

```python
# src/evalvault/domain/metrics/insurance_metrics.py

from ragas.metrics import AspectCritic

class InsuranceTermAccuracy(AspectCritic):
    """ë³´í—˜ ìš©ì–´ ì •í™•ì„± í‰ê°€ ë©”íŠ¸ë¦­.

    ë³´í—˜ ë‹µë³€ì—ì„œ ì‚¬ìš©ëœ ì „ë¬¸ ìš©ì–´ê°€ ì •í™•í•œì§€ í‰ê°€í•©ë‹ˆë‹¤.
    - ë³´í—˜ë£Œ, ë³´í—˜ê¸ˆ, ë©´ì±…ì‚¬í•­, ë³´ì¥ë²”ìœ„ ë“±
    """

    name = "insurance_term_accuracy"

    definition = """
    ë³´í—˜ ê´€ë ¨ ë‹µë³€ì—ì„œ ì „ë¬¸ ìš©ì–´ê°€ ì •í™•í•˜ê²Œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.

    í‰ê°€ ê¸°ì¤€:
    1. ë³´í—˜ ìš©ì–´ì˜ ì •í™•í•œ ì •ì˜ ì‚¬ìš©
    2. ë²•ì /ê·œì œì  ìš©ì–´ì˜ ì˜¬ë°”ë¥¸ ì ìš©
    3. ìˆ˜ì¹˜ ì •ë³´(ë³´í—˜ë£Œ, ë³´ì¥ê¸ˆì•¡)ì˜ ì •í™•ì„±
    """


class RegulatoryCompliance(AspectCritic):
    """ê·œì œ ì¤€ìˆ˜ ì—¬ë¶€ í‰ê°€ ë©”íŠ¸ë¦­.

    ë‹µë³€ì´ ë³´í—˜ ê´€ë ¨ ê·œì œë¥¼ ì¤€ìˆ˜í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.
    - ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²•
    - ë³´í—˜ì—…ë²•
    - ì•½ê´€ ì„¤ëª…ì˜ë¬´
    """

    name = "regulatory_compliance"

    definition = """
    ë³´í—˜ ê´€ë ¨ ë‹µë³€ì´ ê·œì œ ìš”ê±´ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.

    í‰ê°€ ê¸°ì¤€:
    1. ì¤‘ìš”ì‚¬í•­ ê³ ì§€ ì—¬ë¶€
    2. ë©´ì±…ì‚¬í•­ ì„¤ëª… í¬í•¨ ì—¬ë¶€
    3. ì†Œë¹„ì ê¶Œë¦¬ ì•ˆë‚´ í¬í•¨ ì—¬ë¶€
    """


class DisclaimerPresence(AspectCritic):
    """ë©´ì±…ì‚¬í•­/ì£¼ì˜ì‚¬í•­ í¬í•¨ ì—¬ë¶€ ë©”íŠ¸ë¦­.

    ë³´í—˜ ê´€ë ¨ ë‹µë³€ì— í•„ìˆ˜ ë©´ì±…ì‚¬í•­ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """

    name = "disclaimer_presence"

    definition = """
    ë³´í—˜ ê´€ë ¨ ë‹µë³€ì— ì ì ˆí•œ ë©´ì±…ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì‚¬í•­ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.
    """
```

### 3.3 Rubrics ê¸°ë°˜ í‰ê°€

ë³´í—˜ ë„ë©”ì¸ íŠ¹í™” í‰ê°€ë¥¼ ìœ„í•œ rubrics ì •ì˜:

```python
# src/evalvault/domain/metrics/insurance_rubrics.py

INSURANCE_ANSWER_RUBRIC = {
    1: "ë‹µë³€ì´ ì™„ì „íˆ ë¶€ì •í™•í•˜ê±°ë‚˜ ìœ„í—˜í•œ ì •ë³´ë¥¼ í¬í•¨",
    2: "ë‹µë³€ì— ì¤‘ìš”í•œ ëˆ„ë½ì´ ìˆê±°ë‚˜ ë¶€ë¶„ì ìœ¼ë¡œ ë¶€ì •í™•",
    3: "ë‹µë³€ì´ ëŒ€ì²´ë¡œ ì •í™•í•˜ë‚˜ ë©´ì±…ì‚¬í•­/ì£¼ì˜ì‚¬í•­ ëˆ„ë½",
    4: "ë‹µë³€ì´ ì •í™•í•˜ê³  í•„ìš”í•œ ì •ë³´ ëŒ€ë¶€ë¶„ í¬í•¨",
    5: "ë‹µë³€ì´ ì™„ì „íˆ ì •í™•í•˜ê³  ë©´ì±…ì‚¬í•­, ì¶”ê°€ ì•ˆë‚´ê¹Œì§€ í¬í•¨"
}

INSURANCE_CONTEXT_RUBRIC = {
    1: "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì „í˜€ ë¬´ê´€",
    2: "ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ìˆìœ¼ë‚˜ í•µì‹¬ ì •ë³´ ëˆ„ë½",
    3: "ì»¨í…ìŠ¤íŠ¸ê°€ ê´€ë ¨ìˆìœ¼ë‚˜ ìµœì‹  ì •ë³´ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ",
    4: "ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘",
    5: "ì»¨í…ìŠ¤íŠ¸ê°€ ì™„ë²½í•˜ê²Œ ì§ˆë¬¸ì„ ì»¤ë²„í•˜ê³  ê´€ë ¨ ê·œì •ê¹Œì§€ í¬í•¨"
}
```

---

## 4. Testset Generation ì „ëµ

RAGASì˜ Knowledge Graph ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±ì„ ë³´í—˜ ë„ë©”ì¸ì— ì ìš©í•©ë‹ˆë‹¤.

### 4.1 ë³´í—˜ ë¬¸ì„œ ì¿¼ë¦¬ ìœ í˜•

```
ë³´í—˜ ë¬¸ì„œ ì¿¼ë¦¬ ìœ í˜•
â”œâ”€â”€ Single-Hop Query (ë‹¨ì¼ ë¬¸ì„œ)
â”‚   â”œâ”€â”€ Specific: "ì´ ë³´í—˜ì˜ ë³´ì¥ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?"
â”‚   â””â”€â”€ Abstract: "ì´ ë³´í—˜ ìƒí’ˆì˜ ì¥ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
â”‚
â””â”€â”€ Multi-Hop Query (ë³µìˆ˜ ë¬¸ì„œ)
    â”œâ”€â”€ Specific: "Aë³´í—˜ê³¼ Bë³´í—˜ì˜ ë³´ì¥ê¸ˆì•¡ ì°¨ì´ëŠ”?"
    â””â”€â”€ Abstract: "ì¢…í•©ë³´í—˜ê³¼ ë‹¨ë…ë³´í—˜ ì¤‘ ì–´ë–¤ ê²ƒì´ ìœ ë¦¬í•œê°€ìš”?"
```

### 4.2 Knowledge Graph êµ¬ì¶•

```python
# src/evalvault/testset/insurance_kg.py

from ragas.testset.graph import Node, KnowledgeGraph
from ragas.testset.transforms import (
    Parallel,
    apply_transforms,
)
from ragas.testset.transforms.extractors import (
    NERExtractor,
    KeyphraseExtractor,
)

class InsuranceKnowledgeGraphBuilder:
    """ë³´í—˜ ë¬¸ì„œìš© Knowledge Graph ë¹Œë”."""

    def __init__(self, llm):
        self.llm = llm
        self.extractors = self._setup_extractors()

    def _setup_extractors(self):
        """ë³´í—˜ ë„ë©”ì¸ íŠ¹í™” ì¶”ì¶œê¸° ì„¤ì •."""
        return Parallel(
            # ë³´í—˜ ìš©ì–´ ì¶”ì¶œ
            NERExtractor(entity_types=["INSURANCE_TERM", "MONEY", "DATE"]),
            # í•µì‹¬ ë¬¸êµ¬ ì¶”ì¶œ
            KeyphraseExtractor(),
            # ì»¤ìŠ¤í…€: ë³´í—˜ ìƒí’ˆëª… ì¶”ì¶œ
            InsuranceProductExtractor(),
        )

    async def build(self, documents: list[str]) -> KnowledgeGraph:
        """ë¬¸ì„œë¡œë¶€í„° Knowledge Graph êµ¬ì¶•."""
        nodes = [
            Node(properties={"page_content": doc})
            for doc in documents
        ]

        kg = KnowledgeGraph(nodes=nodes)

        transforms = [
            self.extractors,
            InsuranceRelationshipBuilder(),
        ]

        await apply_transforms(kg, transforms)
        return kg


class InsuranceProductExtractor:
    """ë³´í—˜ ìƒí’ˆëª… ë° ìœ í˜• ì¶”ì¶œê¸°."""

    async def extract(self, node):
        # ë³´í—˜ ìƒí’ˆ ìœ í˜• íŒ¨í„´
        patterns = [
            r"(ì¢…ì‹ |ì •ê¸°|ë³€ì•¡|ì—°ê¸ˆ|ê±´ê°•|ì‹¤ì†|ìë™ì°¨|í™”ì¬|ë°°ìƒì±…ì„)\s*ë³´í—˜",
            r"(í”Œëœ|íŠ¹ì•½|ë‹´ë³´)",
        ]
        # ... ì¶”ì¶œ ë¡œì§
        return ("insurance_products", extracted_products)
```

### 4.3 ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±

```python
# src/evalvault/testset/insurance_synthesizer.py

from ragas.testset.synthesizers.base_query import QuerySynthesizer
from dataclasses import dataclass

@dataclass
class InsuranceQuerySynthesizer(QuerySynthesizer):
    """ë³´í—˜ ë„ë©”ì¸ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±ê¸°."""

    # ë³´í—˜ ê³ ê° í˜ë¥´ì†Œë‚˜
    personas = [
        {"name": "ì‹ ê·œ ê°€ì…ì", "traits": "ë³´í—˜ ì´ˆë³´, ìš©ì–´ì— ìµìˆ™í•˜ì§€ ì•ŠìŒ"},
        {"name": "ê¸°ì¡´ ê°€ì…ì", "traits": "ê°±ì‹ /ë³€ê²½ ê´€ì‹¬, ë¹„êµ ë¶„ì„ ì„ í˜¸"},
        {"name": "ì²­êµ¬ ê³ ê°", "traits": "ë³´í—˜ê¸ˆ ì²­êµ¬ ì ˆì°¨ ë¬¸ì˜, êµ¬ì²´ì  ì§ˆë¬¸"},
        {"name": "í•´ì§€ ê²€í† ì", "traits": "ë¶ˆë§Œì¡±, ë¹„ìš© ë¯¼ê°, ëŒ€ì•ˆ íƒìƒ‰"},
    ]

    # ì¿¼ë¦¬ ìŠ¤íƒ€ì¼
    query_styles = [
        "formal",      # ê³µì‹ì  ë¬¸ì˜
        "casual",      # ì¼ìƒì  ëŒ€í™”
        "urgent",      # ê¸´ê¸‰ ë¬¸ì˜
        "comparison",  # ë¹„êµ ì§ˆë¬¸
    ]

    async def _generate_scenarios(self, n, knowledge_graph, callbacks):
        """ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±."""
        scenarios = []

        # KGì—ì„œ ê´€ë ¨ ë…¸ë“œ ì¡°í•© íƒìƒ‰
        for node_pair in knowledge_graph.get_related_nodes():
            for persona in self.personas:
                for style in self.query_styles:
                    scenarios.append({
                        "nodes": node_pair,
                        "persona": persona,
                        "style": style,
                        "language": "ko",  # ê¸°ë³¸ í•œêµ­ì–´
                    })

        return scenarios[:n]

    async def _generate_sample(self, scenario, callbacks):
        """ì‹œë‚˜ë¦¬ì˜¤ë¡œë¶€í„° í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìƒì„±."""
        # LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ + ê¸°ëŒ€ ë‹µë³€ ìƒì„±
        query = await self._synthesize_query(scenario)
        reference = await self._synthesize_reference(scenario)

        return SingleTurnSample(
            user_input=query,
            reference_contexts=[n.properties["page_content"] for n in scenario["nodes"]],
            reference=reference,
        )
```

### 4.4 êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ë‹¨ê³„ | ì‘ì—… | ì‚°ì¶œë¬¼ |
|-----|------|--------|
| 1 | ê¸°ë³¸ ë¬¸ì„œ íŒŒì„œ | `InsuranceDocumentParser` |
| 2 | ì—”í‹°í‹° ì¶”ì¶œê¸° | `InsuranceProductExtractor`, `InsuranceTermExtractor` |
| 3 | KG ë¹Œë” | `InsuranceKnowledgeGraphBuilder` |
| 4 | ì¿¼ë¦¬ ìƒì„±ê¸° | `InsuranceQuerySynthesizer` |
| 5 | CLI í†µí•© | `evalvault testset generate` ëª…ë ¹ |

---

## 5. Experiment ê´€ë¦¬ ì „ëµ

### 5.1 RAGAS @experiment ë°ì½”ë ˆì´í„° í†µí•©

```python
# src/evalvault/experiments/runner.py

from ragas import experiment, Dataset
from datetime import datetime

@experiment()
async def insurance_rag_experiment(row, model_name: str, retriever_type: str):
    """ë³´í—˜ RAG ì‹œìŠ¤í…œ ì‹¤í—˜.

    Args:
        row: ë°ì´í„°ì…‹ í–‰
        model_name: ì‚¬ìš©í•  LLM ëª¨ë¸
        retriever_type: ê²€ìƒ‰ê¸° ìœ í˜• (dense, sparse, hybrid)
    """
    # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    response = await run_insurance_rag_pipeline(
        query=row["user_input"],
        model=model_name,
        retriever=retriever_type,
    )

    return {
        **row,
        "response": response.answer,
        "retrieved_contexts": response.contexts,
        "experiment_name": f"{model_name}_{retriever_type}_{datetime.now():%Y%m%d}",
        "model_name": model_name,
        "retriever_type": retriever_type,
        "latency_ms": response.latency_ms,
        "tokens_used": response.tokens_used,
    }
```

### 5.2 ì‹¤í—˜ ë¹„êµ ë° ë¶„ì„

```python
# src/evalvault/experiments/analysis.py

class ExperimentAnalyzer:
    """ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„ê¸°."""

    def compare_experiments(
        self,
        experiment_ids: list[str],
        metrics: list[str] = None,
    ) -> ComparisonReport:
        """ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

        Returns:
            ê° ë©”íŠ¸ë¦­ë³„ í‰ê· , í‘œì¤€í¸ì°¨, í†µê³„ì  ìœ ì˜ì„± í¬í•¨ ë³´ê³ ì„œ
        """
        pass

    def detect_regression(
        self,
        baseline_id: str,
        candidate_id: str,
        threshold: float = 0.05,
    ) -> RegressionReport:
        """ì„±ëŠ¥ íšŒê·€ ê°ì§€.

        Returns:
            íšŒê·€ ë°œìƒ ë©”íŠ¸ë¦­ ë° ì‹¬ê°ë„ ë³´ê³ ì„œ
        """
        pass
```

### 5.3 ì‹¤í—˜ ê²°ê³¼ ì €ì¥ êµ¬ì¡°

```
experiments/
â”œâ”€â”€ 20241224-143022-gpt4o_dense_baseline.csv
â”œâ”€â”€ 20241224-150515-gpt4o_hybrid_v1.csv
â”œâ”€â”€ 20241224-160000-claude_dense_comparison.csv
â””â”€â”€ metadata/
    â”œâ”€â”€ experiment_registry.json
    â””â”€â”€ comparison_reports/
        â””â”€â”€ 20241224_baseline_vs_hybrid.json
```

---

## 6. Langfuse í†µí•© ê°•í™”

### 6.1 í˜„ì¬ êµ¬í˜„ ìƒíƒœ

- âœ… ê¸°ë³¸ Trace ìƒì„±
- âœ… Span ì¶”ê°€
- âœ… Score ë¡œê¹…
- âœ… EvaluationRun ì „ì²´ ë¡œê¹…

### 6.2 ì¶”ê°€ í†µí•© í•„ìš” ì‚¬í•­

#### 6.2.1 Ragas-Langfuse ë„¤ì´í‹°ë¸Œ í†µí•©

```python
# src/evalvault/adapters/outbound/tracker/langfuse_ragas_adapter.py

from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
from ragas.metrics import Faithfulness

class LangfuseRagasIntegration:
    """Ragas + Langfuse ë„¤ì´í‹°ë¸Œ í†µí•©."""

    def __init__(self, langfuse_client: Langfuse):
        self.langfuse = langfuse_client

    @observe(name="ragas_evaluation")
    async def evaluate_with_tracing(
        self,
        sample,
        metrics: list,
    ):
        """Langfuse traceì™€ í•¨ê»˜ Ragas í‰ê°€ ì‹¤í–‰."""
        results = {}

        for metric in metrics:
            with langfuse_context.update_current_trace(
                metadata={"metric": metric.name}
            ):
                score = await metric.single_turn_ascore(sample)

                # ìë™ìœ¼ë¡œ Langfuseì— score ê¸°ë¡
                langfuse_context.score_current_trace(
                    name=metric.name,
                    value=score,
                )

                results[metric.name] = score

        return results
```

#### 6.2.2 Dataset ê´€ë¦¬ í†µí•©

```python
# Langfuse Datasetê³¼ EvalVault Dataset ì—°ë™

class LangfuseDatasetSync:
    """Langfuseì™€ EvalVault ë°ì´í„°ì…‹ ë™ê¸°í™”."""

    async def push_to_langfuse(self, dataset: Dataset) -> str:
        """EvalVault ë°ì´í„°ì…‹ì„ Langfuseì— ì—…ë¡œë“œ."""
        lf_dataset = self.langfuse.create_dataset(
            name=f"{dataset.name}_v{dataset.version}",
            description=f"Insurance RAG evaluation dataset",
        )

        for test_case in dataset.test_cases:
            lf_dataset.add_item(
                input={"query": test_case.question},
                expected_output=test_case.ground_truth,
                metadata=test_case.metadata,
            )

        return lf_dataset.id

    async def pull_from_langfuse(self, dataset_name: str) -> Dataset:
        """Langfuse ë°ì´í„°ì…‹ì„ EvalVaultë¡œ ê°€ì ¸ì˜¤ê¸°."""
        pass
```

### 6.3 ëŒ€ì‹œë³´ë“œ ì—°ë™

Langfuseì˜ ê¸°ë³¸ ëŒ€ì‹œë³´ë“œ ì™¸ì— ì»¤ìŠ¤í…€ ë·° ì„¤ì •:

1. **Evaluation Overview**: ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½
2. **Metric Trends**: ì‹œê°„ì— ë”°ë¥¸ ë©”íŠ¸ë¦­ ë³€í™”
3. **Test Case Details**: ê°œë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„
4. **Model Comparison**: ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ

---

## 7. êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ ê°•í™” (í˜„ì¬)

```
Week 1-2:
â”œâ”€â”€ [x] ê¸°ë³¸ Ragas í†µí•©
â”œâ”€â”€ [x] Langfuse ì–´ëŒ‘í„°
â”œâ”€â”€ [ ] ì–¸ì–´ ê°ì§€ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ [ ] í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
â””â”€â”€ [ ] FactualCorrectness ë©”íŠ¸ë¦­ ì¶”ê°€
```

### Phase 2: ë„ë©”ì¸ íŠ¹í™”

```
Week 3-4:
â”œâ”€â”€ [ ] InsuranceTermAccuracy ë©”íŠ¸ë¦­
â”œâ”€â”€ [ ] RegulatoryCompliance ë©”íŠ¸ë¦­
â”œâ”€â”€ [ ] ë³´í—˜ ë„ë©”ì¸ Rubrics ì •ì˜
â””â”€â”€ [ ] ìƒ˜í”Œ ë³´í—˜ ë°ì´í„°ì…‹ ìƒì„±
```

### Phase 3: Testset Generation

```
Week 5-6:
â”œâ”€â”€ [ ] InsuranceDocumentParser
â”œâ”€â”€ [ ] Knowledge Graph ë¹Œë”
â”œâ”€â”€ [ ] InsuranceQuerySynthesizer
â””â”€â”€ [ ] CLI í†µí•© (evalvault testset generate)
```

### Phase 4: ì‹¤í—˜ ê´€ë¦¬

```
Week 7-8:
â”œâ”€â”€ [ ] @experiment ë°ì½”ë ˆì´í„° í†µí•©
â”œâ”€â”€ [ ] ì‹¤í—˜ ë¹„êµ ë¶„ì„ê¸°
â”œâ”€â”€ [ ] Langfuse ë„¤ì´í‹°ë¸Œ í†µí•©
â””â”€â”€ [ ] ëŒ€ì‹œë³´ë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•
```

---

## 8. íŒŒì¼ êµ¬ì¡° (ëª©í‘œ)

```
src/evalvault/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”œâ”€â”€ dataset.py          # âœ… ì™„ë£Œ
â”‚   â”‚   â””â”€â”€ result.py           # âœ… ì™„ë£Œ
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ evaluator.py        # âœ… ê¸°ë³¸ ì™„ë£Œ
â”‚   â”œâ”€â”€ metrics/                 # ğŸ†• ì‹ ê·œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bilingual.py        # ë‹¤êµ­ì–´ ë©”íŠ¸ë¦­
â”‚   â”‚   â”œâ”€â”€ insurance.py        # ë³´í—˜ íŠ¹í™” ë©”íŠ¸ë¦­
â”‚   â”‚   â””â”€â”€ rubrics.py          # Rubrics ì •ì˜
â”‚   â””â”€â”€ prompts/                 # ğŸ†• ì‹ ê·œ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ korean.py           # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸
â”‚       â””â”€â”€ insurance.py        # ë³´í—˜ ë„ë©”ì¸ í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ testset/                     # ğŸ†• ì‹ ê·œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ insurance.py        # ë³´í—˜ ì—”í‹°í‹° ì¶”ì¶œê¸°
â”‚   â”œâ”€â”€ kg_builder.py           # Knowledge Graph ë¹Œë”
â”‚   â””â”€â”€ synthesizer.py          # ì¿¼ë¦¬ ìƒì„±ê¸°
â”œâ”€â”€ experiments/                 # ğŸ†• ì‹ ê·œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ runner.py               # ì‹¤í—˜ ì‹¤í–‰ê¸°
â”‚   â””â”€â”€ analyzer.py             # ê²°ê³¼ ë¶„ì„ê¸°
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ outbound/
â”‚   â”‚   â””â”€â”€ tracker/
â”‚   â”‚       â””â”€â”€ langfuse_adapter.py  # âœ… ì™„ë£Œ (í™•ì¥ ì˜ˆì •)
â”‚   â””â”€â”€ inbound/
â”‚       â””â”€â”€ cli.py              # âœ… ê¸°ë³¸ ì™„ë£Œ (í™•ì¥ ì˜ˆì •)
â””â”€â”€ utils/                       # ğŸ†• ì‹ ê·œ
    â”œâ”€â”€ __init__.py
    â””â”€â”€ language.py             # ì–¸ì–´ ê°ì§€ ìœ í‹¸ë¦¬í‹°
```

---

## 9. ì„±ê³µ ê¸°ì¤€ (SLA)

### 9.1 ë©”íŠ¸ë¦­ ì„ê³„ê°’

| ë©”íŠ¸ë¦­ | ìµœì†Œ ê¸°ì¤€ | ëª©í‘œ | ìš°ìˆ˜ |
|--------|----------|------|------|
| Faithfulness | 0.60 | 0.80 | 0.90 |
| Answer Relevancy | 0.65 | 0.80 | 0.90 |
| Context Precision | 0.60 | 0.75 | 0.85 |
| Context Recall | 0.60 | 0.80 | 0.90 |
| FactualCorrectness | 0.70 | 0.85 | 0.95 |
| InsuranceTermAccuracy | 0.75 | 0.90 | 0.95 |

### 9.2 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **í‰ê°€ ì²˜ë¦¬ëŸ‰**: 100 test cases / 5ë¶„ ì´ë‚´
- **ê²°ê³¼ ì €ì¥**: ëª¨ë“  ê²°ê³¼ SQLite + Langfuse ì´ì¤‘ ì €ì¥
- **ì¬í˜„ì„±**: ë™ì¼ ì…ë ¥ â†’ ë™ì¼ ê²°ê³¼ (temperature=0)

---

## 10. ì°¸ê³  ìë£Œ

- [RAGAS Documentation](https://docs.ragas.io/)
- [Langfuse Documentation](https://langfuse.com/docs)
- [ê¸ˆìœµìœ„ì›íšŒ ë³´í—˜ì—…ë²•](https://www.law.go.kr/)
- [ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²•](https://www.law.go.kr/)
